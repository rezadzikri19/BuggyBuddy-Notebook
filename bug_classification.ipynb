{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msi.pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msi.pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "import concurrent.futures\n",
    "import asyncio\n",
    "import httpx\n",
    "import multiprocessing\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "from typing import List, Dict, Optional, Union\n",
    "from glob import glob\n",
    "\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Lambda\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url: str, params=None):\n",
    "  try:\n",
    "    response = requests.get(url, params=params)\n",
    "    if (response.status_code == 200):\n",
    "      return response.json()\n",
    "    \n",
    "    print(f\"Error response with status code: {response.status_code}\")\n",
    "  except Exception as error:\n",
    "    print(f'Failed to fetch data: {error}')\n",
    "\n",
    "def urls_builder(base_url: str, n_fetch: int, limit: int, products: List[str], **kwargs):\n",
    "  urls = []\n",
    "  for product in products:\n",
    "    for i in range(n_fetch):\n",
    "      \n",
    "      param = {\n",
    "        'offset': i * limit,\n",
    "        'limit': limit,\n",
    "        'product': product,\n",
    "        **kwargs,\n",
    "      }\n",
    "      \n",
    "      full_url = base_url + '?' + urlencode(param)\n",
    "      urls.append(full_url)\n",
    "    \n",
    "  return urls\n",
    "\n",
    "def save_json(data, path: str):\n",
    "  with open(path, 'w') as json_file:\n",
    "    json.dump(data, json_file)\n",
    "\n",
    "def load_json(path: str):\n",
    "  with open(path, 'r') as json_file:\n",
    "    loaded_data = json.load(json_file)\n",
    "  return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script skipped # Skipped due to resource optimization\n",
    "\n",
    "selected_fields = ['id', 'duplicates', 'summary', 'description', 'status', 'resolution', 'platform', 'product', 'type', 'priority', 'severity', 'component']\n",
    "products = ['Core']\n",
    "\n",
    "base_params = {\n",
    "  'include_fields': ', '.join(selected_fields),\n",
    "}\n",
    "saved_data_path = os.path.join('data', 'raw_data', 'core_raw_data.json')\n",
    "\n",
    "base_url = 'https://bugzilla.mozilla.org/rest/bug'\n",
    "n_fetch = 50\n",
    "limit = 5000\n",
    "\n",
    "if os.path.exists(saved_data_path):\n",
    "  response_data = load_json(saved_data_path)\n",
    "else:\n",
    "  urls = urls_builder(base_url, n_fetch, limit, products, **base_params)\n",
    "  response_data = []\n",
    "\n",
    "  max_workers = 50\n",
    "  with concurrent.futures.ThreadPoolExecutor(max_workers) as executor:\n",
    "    response_data = list(executor.map(fetch_data, urls))\n",
    "    \n",
    "  response_data = [item['bugs'] for item in response_data]\n",
    "  response_data = [item for sublist in response_data for item in sublist]\n",
    "  \n",
    "  save_json(response_data, saved_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>summary</th>\n",
       "      <th>status</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>priority</th>\n",
       "      <th>resolution</th>\n",
       "      <th>product</th>\n",
       "      <th>severity</th>\n",
       "      <th>component</th>\n",
       "      <th>duplicates</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>406067</th>\n",
       "      <td>x86</td>\n",
       "      <td>Chrome error</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>defect</td>\n",
       "      <td>Weave currently generates the following error ...</td>\n",
       "      <td>--</td>\n",
       "      <td>FIXED</td>\n",
       "      <td>Cloud Services</td>\n",
       "      <td>minor</td>\n",
       "      <td>General</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408325</th>\n",
       "      <td>x86</td>\n",
       "      <td>Can no longer synch between two machines runni...</td>\n",
       "      <td>VERIFIED</td>\n",
       "      <td>defect</td>\n",
       "      <td>Since just after 8:12 PM on 12/11, I've been g...</td>\n",
       "      <td>--</td>\n",
       "      <td>FIXED</td>\n",
       "      <td>Cloud Services</td>\n",
       "      <td>normal</td>\n",
       "      <td>General</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409568</th>\n",
       "      <td>All</td>\n",
       "      <td>Arrow not pointing to weave icon on services.m...</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>defect</td>\n",
       "      <td>See screenshot. The arrow is not pointing to t...</td>\n",
       "      <td>--</td>\n",
       "      <td>DUPLICATE</td>\n",
       "      <td>Cloud Services</td>\n",
       "      <td>minor</td>\n",
       "      <td>General</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409579</th>\n",
       "      <td>All</td>\n",
       "      <td>Mail from Weave detected as spam</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>defect</td>\n",
       "      <td>The confirmation mail that Weave sent me was d...</td>\n",
       "      <td>--</td>\n",
       "      <td>FIXED</td>\n",
       "      <td>Cloud Services</td>\n",
       "      <td>major</td>\n",
       "      <td>General</td>\n",
       "      <td>[409713]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409600</th>\n",
       "      <td>All</td>\n",
       "      <td>First run page has an arrow that points to the...</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>defect</td>\n",
       "      <td>The Weave extension's firstrun page has an arr...</td>\n",
       "      <td>--</td>\n",
       "      <td>WONTFIX</td>\n",
       "      <td>Cloud Services</td>\n",
       "      <td>normal</td>\n",
       "      <td>General</td>\n",
       "      <td>[409568, 420386]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       platform                                            summary    status  \\\n",
       "id                                                                             \n",
       "406067      x86                                       Chrome error  RESOLVED   \n",
       "408325      x86  Can no longer synch between two machines runni...  VERIFIED   \n",
       "409568      All  Arrow not pointing to weave icon on services.m...  RESOLVED   \n",
       "409579      All                   Mail from Weave detected as spam  RESOLVED   \n",
       "409600      All  First run page has an arrow that points to the...  RESOLVED   \n",
       "\n",
       "          type                                        description priority  \\\n",
       "id                                                                           \n",
       "406067  defect  Weave currently generates the following error ...       --   \n",
       "408325  defect  Since just after 8:12 PM on 12/11, I've been g...       --   \n",
       "409568  defect  See screenshot. The arrow is not pointing to t...       --   \n",
       "409579  defect  The confirmation mail that Weave sent me was d...       --   \n",
       "409600  defect  The Weave extension's firstrun page has an arr...       --   \n",
       "\n",
       "       resolution         product severity component        duplicates  \n",
       "id                                                                      \n",
       "406067      FIXED  Cloud Services    minor   General                []  \n",
       "408325      FIXED  Cloud Services   normal   General                []  \n",
       "409568  DUPLICATE  Cloud Services    minor   General                []  \n",
       "409579      FIXED  Cloud Services    major   General          [409713]  \n",
       "409600    WONTFIX  Cloud Services   normal   General  [409568, 420386]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_path = os.path.join('data', 'cache', 'raw_data.parquet')\n",
    "df = pd.DataFrame()\n",
    "\n",
    "if (os.path.exists(raw_data_path)):\n",
    "  df = pd.read_parquet(raw_data_path)\n",
    "else:\n",
    "  data_paths = glob(os.path.join('data', 'raw_data', '*.json'))\n",
    "\n",
    "  for path in data_paths:\n",
    "    data = load_json(path)\n",
    "    data = pd.DataFrame(data)\n",
    "    df = pd.concat([df, data])\n",
    "\n",
    "  df = df.set_index('id')\n",
    "  df.to_parquet(raw_data_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['status', 'priority', 'resolution', 'severity', 'component'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1131641 entries, 406067 to 1876207\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   platform     1131641 non-null  object\n",
      " 1   summary      1131641 non-null  object\n",
      " 2   type         1131641 non-null  object\n",
      " 3   description  1131641 non-null  object\n",
      " 4   product      1131641 non-null  object\n",
      " 5   duplicates   1131641 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 1.9 GB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1. Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['product'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3. Platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['platform'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4. Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5. Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df['duplicates'].apply(lambda x: len(x)).sort_values(ascending=False)\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_bugs = duplicates[duplicates > 0]\n",
    "duplicated_bugs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1. Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['type'] == '--', 'type'] = 'no type'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2. Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = df.loc[duplicated_bugs.index, 'duplicates']\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('data', 'cache', 'data.parquet')\n",
    "\n",
    "if (os.path.exists(data_path)):\n",
    "  df = pd.read_parquet(data_path)\n",
    "else:\n",
    "  df['duplicates_to'] = -1\n",
    "\n",
    "  for idx, dups in zip(duplicated.index, duplicated):\n",
    "    for item in dups:\n",
    "      df.loc[df.index == item, 'duplicates_to'] = idx\n",
    "      \n",
    "  df.to_parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['duplicates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3. Clean Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text: str):\n",
    "  text = text.lower()\n",
    "  text = re.sub(r'\\n|\\t|\\r|\\0', ' ', text)\n",
    "  text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "  text = re.sub(r'\\s{2,}', ' ', text)\n",
    "  text = re.sub(r'\\s$', '', text)\n",
    "  text = re.sub(r'\\s[b-z]\\s', ' ', text)\n",
    "  text = re.sub(r'\\s[b-z]\\s', ' ', text)\n",
    "  \n",
    "  return text\n",
    "\n",
    "def remove_stopwords(text: str):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = word_tokenize(text)\n",
    "  filtered_words = [word for word in words if word not in stop_words]\n",
    "  \n",
    "  return ' '.join(filtered_words)\n",
    "\n",
    "def clean_text(text: str):\n",
    "  text = remove_special_chars(text)\n",
    "  text = remove_stopwords(text)\n",
    "  \n",
    "  return text\n",
    "\n",
    "df['platform'] = df['platform'].apply(clean_text)\n",
    "df['summary'] = df['summary'].apply(clean_text)\n",
    "df['type'] = df['type'].apply(clean_text)\n",
    "df['description'] = df['description'].apply(clean_text)\n",
    "df['product'] = df['product'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4. Combined Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['type'] + ' ' + df['platform'] + ' ' + df['product'] + ' ' + df['summary'] + ' ' + df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'duplicates_to']]\n",
    "gc.collect()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.5. Sentence Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embd_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_data_path = os.path.join('data', 'cache', 'embd_data.parquet')\n",
    "\n",
    "if (os.path.exists(embd_data_path)):\n",
    "  df = pd.read_parquet(embd_data_path)\n",
    "else:\n",
    "  df['text_embedded'] = df['text'].apply(sent_embd_model.encode)\n",
    "  df.to_parquet(embd_data_path)\n",
    "\n",
    "df['text_embedded'] = df['text_embedded'].apply(np.array)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = df[['text']]\n",
    "df = df.drop(columns=['text'])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.6. Sentence Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df['_id'] = df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniques = df[df['duplicates_to'] == -1].copy()\n",
    "df_duplicates = df[df['duplicates_to'] != -1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cosine_similarity(text_1: pd.Series, text_2: pd.Series):\n",
    "    dot_product = np.dot(text_1, text_2)\n",
    "    norm_text_1 = np.linalg.norm(text_1)\n",
    "    norm_text_2 = np.linalg.norm(text_2)\n",
    "    \n",
    "    similarity = dot_product / (norm_text_1 * norm_text_2)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicates = pd.merge(left=df_duplicates,\n",
    "                        right=df_uniques,\n",
    "                        left_on='duplicates_to',\n",
    "                        right_on='id',\n",
    "                        suffixes=('__left', '__right'))\n",
    "df_duplicates = df_duplicates[['text_embedded__left', 'text_embedded__right', '_id__left', '_id__right']]\n",
    "df_duplicates['label'] = 1\n",
    "df_duplicates = df_duplicates.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicates['cosine_similarity'] = df_duplicates.apply(lambda x: custom_cosine_similarity(x['text_embedded__left'], x['text_embedded__right']), axis=1)\n",
    "\n",
    "df_duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_rows = df_uniques.shape[0] // 2\n",
    "\n",
    "df_uniques_temp = pd.DataFrame()\n",
    "df_uniques_temp['text_embedded__left'] = df_uniques['text_embedded']\n",
    "df_uniques_temp['_id__left'] = df_uniques['_id']\n",
    "df_uniques_temp['text_embedded__right'] = df_uniques['text_embedded'].shift(100)\n",
    "df_uniques_temp['_id__right'] = df_uniques['_id'].shift(100)\n",
    "df_uniques_temp = df_uniques_temp.dropna()\n",
    "df_uniques_temp['label'] = 0\n",
    "\n",
    "df_uniques = df_uniques_temp.reset_index(drop=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniques['cosine_similarity'] = df_uniques.apply(lambda x: custom_cosine_similarity(x['text_embedded__left'], x['text_embedded__right']), axis=1)\n",
    "\n",
    "df_uniques.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "\n",
    "unique = df_uniques['cosine_similarity'][df_uniques['cosine_similarity'] > threshold].count() / df_uniques['cosine_similarity'].count()\n",
    "duplicate = df_duplicates['cosine_similarity'][df_duplicates['cosine_similarity'] > threshold].count() / df_duplicates['cosine_similarity'].count()\n",
    "\n",
    "print(unique * 100, duplicate * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_duplicates, df_uniques], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Data Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1. Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['label']),\n",
    "                                                    df['label'],\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=df['label'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(384, ))\n",
    "\n",
    "x1 = Dense(128, activation=\"relu\")(inputs)\n",
    "x1 = Dense(64, activation=\"relu\")(x1)\n",
    "x1 = Dense(32, activation=\"relu\")(x1)\n",
    "\n",
    "x2 = Dense(128, activation=\"relu\")(inputs)\n",
    "x2 = Dense(64, activation=\"relu\")(x2)\n",
    "x2 = Dense(32, activation=\"relu\")(x2)\n",
    "\n",
    "distance = Lambda(lambda x: tf.abs(x[0] - x[1]))([x1, x2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
