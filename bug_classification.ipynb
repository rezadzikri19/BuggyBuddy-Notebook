{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msi.pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msi.pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "import concurrent.futures\n",
    "import asyncio\n",
    "import httpx\n",
    "import multiprocessing\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "from typing import List, Dict, Optional, Union\n",
    "from glob import glob\n",
    "\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import fasttext\n",
    "from gensim.models import KeyedVectors \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Input, Lambda, Dot\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url: str, params=None):\n",
    "  try:\n",
    "    response = requests.get(url, params=params)\n",
    "    if (response.status_code == 200):\n",
    "      return response.json()\n",
    "    \n",
    "    print(f\"Error response with status code: {response.status_code}\")\n",
    "  except Exception as error:\n",
    "    print(f'Failed to fetch data: {error}')\n",
    "\n",
    "def urls_builder(base_url: str, n_fetch: int, limit: int, products: List[str], **kwargs):\n",
    "  urls = []\n",
    "  for product in products:\n",
    "    for i in range(n_fetch):\n",
    "      \n",
    "      param = {\n",
    "        'offset': i * limit,\n",
    "        'limit': limit,\n",
    "        'product': product,\n",
    "        **kwargs,\n",
    "      }\n",
    "      \n",
    "      full_url = base_url + '?' + urlencode(param)\n",
    "      urls.append(full_url)\n",
    "    \n",
    "  return urls\n",
    "\n",
    "def save_json(data, path: str):\n",
    "  with open(path, 'w') as json_file:\n",
    "    json.dump(data, json_file)\n",
    "\n",
    "def load_json(path: str):\n",
    "  with open(path, 'r') as json_file:\n",
    "    loaded_data = json.load(json_file)\n",
    "  return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script skipped # Skipped due to resource optimization\n",
    "\n",
    "selected_fields = ['id', 'duplicates', 'summary', 'description', 'status', 'resolution', 'platform', 'product', 'type', 'priority', 'severity', 'component']\n",
    "products = ['Firefox']\n",
    "\n",
    "base_params = {\n",
    "  'include_fields': ', '.join(selected_fields),\n",
    "}\n",
    "saved_data_path = os.path.join('data', 'raw_data', 'firefox_raw_data.json')\n",
    "\n",
    "base_url = 'https://bugzilla.mozilla.org/rest/bug'\n",
    "n_fetch = 50\n",
    "limit = 5000\n",
    "\n",
    "if os.path.exists(saved_data_path):\n",
    "  response_data = load_json(saved_data_path)\n",
    "else:\n",
    "  urls = urls_builder(base_url, n_fetch, limit, products, **base_params)\n",
    "  response_data = []\n",
    "\n",
    "  max_workers = 50\n",
    "  with concurrent.futures.ThreadPoolExecutor(max_workers) as executor:\n",
    "    response_data = list(executor.map(fetch_data, urls))\n",
    "    \n",
    "  response_data = [item['bugs'] for item in response_data]\n",
    "  response_data = [item for sublist in response_data for item in sublist]\n",
    "  \n",
    "  save_json(response_data, saved_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = os.path.join('data', 'cache', 'raw_data.parquet')\n",
    "df = pd.DataFrame()\n",
    "\n",
    "if (os.path.exists(raw_data_path)):\n",
    "  df = pd.read_parquet(raw_data_path)\n",
    "else:\n",
    "  data_paths = glob(os.path.join('data', 'raw_data', 'firefox_raw_data.json'))\n",
    "\n",
    "  for path in data_paths:\n",
    "    data = load_json(path)\n",
    "    data = pd.DataFrame(data)\n",
    "    df = pd.concat([df, data])\n",
    "\n",
    "  df = df.set_index('id')\n",
    "  df.to_parquet(raw_data_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['status', 'priority', 'resolution', 'severity', 'component', 'product'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1. Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script skipped # Skipped due to resource optimization\n",
    "\n",
    "df['product'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3. Platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['platform'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4. Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5. Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df['duplicates'].apply(lambda x: len(x)).sort_values(ascending=False)\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_bugs = duplicates[duplicates > 0]\n",
    "duplicated_bugs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1. Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['type'] == '--', 'type'] = 'no type'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2. Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = df.loc[duplicated_bugs.index, 'duplicates']\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('data', 'cache', 'duplicate_to_data.parquet')\n",
    "\n",
    "if (os.path.exists(data_path)):\n",
    "  df = pd.read_parquet(data_path)\n",
    "else:\n",
    "  df['duplicates_to'] = -1\n",
    "\n",
    "  for idx, dups in zip(duplicated.index, duplicated):\n",
    "    for item in dups:\n",
    "      df.loc[df.index == item, 'duplicates_to'] = idx\n",
    "      \n",
    "  df.to_parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['duplicates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3. Clean Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text: str):\n",
    "  text = text.lower()\n",
    "  text = re.sub(r'\\n|\\t|\\r|\\0', ' ', text)\n",
    "  text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "  text = re.sub(r'\\s{2,}', ' ', text)\n",
    "  text = re.sub(r'\\s$', '', text)\n",
    "  text = re.sub(r'\\s[b-z]\\s', ' ', text)\n",
    "  text = re.sub(r'\\s[b-z]\\s', ' ', text)\n",
    "  \n",
    "  return text\n",
    "\n",
    "def remove_stopwords(text: str):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = word_tokenize(text)\n",
    "  filtered_words = [word for word in words if word not in stop_words]\n",
    "  \n",
    "  return ' '.join(filtered_words)\n",
    "\n",
    "def clean_text(text: str):\n",
    "  text = remove_special_chars(text)\n",
    "  text = remove_stopwords(text)\n",
    "  \n",
    "  return text\n",
    "\n",
    "df['platform'] = df['platform'].apply(clean_text)\n",
    "df['summary'] = df['summary'].apply(clean_text)\n",
    "df['type'] = df['type'].apply(clean_text)\n",
    "df['description'] = df['description'].apply(clean_text)\n",
    "\n",
    "# df['product'] = df['product'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4. Combined Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['type'] + ' ' + df['platform'] + ' ' + df['summary'] + ' ' + df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'duplicates_to']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.5. Sentence Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script skipped # Skipped\n",
    "\n",
    "sent_embd_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%script skipped # Skipped\n",
    "\n",
    "embd_data_path = os.path.join('data', 'cache', 'sent_trans_embd_data.parquet')\n",
    "\n",
    "if (os.path.exists(embd_data_path)):\n",
    "  df = pd.read_parquet(embd_data_path)\n",
    "else:\n",
    "  df['text_embedded'] = df['text'].apply(sent_embd_model.encode)\n",
    "  df.to_parquet(embd_data_path)\n",
    "\n",
    "df['text_embedded'] = df['text_embedded'].apply(np.array)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script skipped # Skipped\n",
    "\n",
    "model_path = os.path.join('models', 'wiki-news-300d-1M-subword.vec')\n",
    "sent_embd_model = KeyedVectors.load_word2vec_format(model_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script skipped # Skipped\n",
    "\n",
    "def sentence_embedding(sentence, model):\n",
    "    words = word_tokenize(sentence)\n",
    "    words_in_vocab = [word for word in words if word in model]\n",
    "\n",
    "    if not words_in_vocab:\n",
    "        return None\n",
    "\n",
    "    embedding = np.mean([model[word] for word in words_in_vocab], axis=0)\n",
    "    embedding = normalize(embedding.reshape(1, -1), norm='l2').reshape(-1)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script skipped # Skipped\n",
    "\n",
    "embd_data_path = os.path.join('data', 'cache', 'fasttext_embd_data.parquet')\n",
    "\n",
    "if (os.path.exists(embd_data_path)):\n",
    "  df = pd.read_parquet(embd_data_path)\n",
    "else:\n",
    "  df['text_embedded'] = df['text'].apply(sentence_embedding, model=sent_embd_model)\n",
    "  df.to_parquet(embd_data_path)\n",
    "\n",
    "df['text_embedded'] = df['text_embedded'].apply(np.array)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.6. Sentence Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicates = df[df['duplicates_to'] != -1].copy()\n",
    "df_uniques = df[df['duplicates_to'] == -1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cosine_similarity(text_1: pd.Series, text_2: pd.Series):\n",
    "    dot_product = np.dot(text_1, text_2)\n",
    "    norm_text_1 = np.linalg.norm(text_1)\n",
    "    norm_text_2 = np.linalg.norm(text_2)\n",
    "    \n",
    "    similarity = dot_product / (norm_text_1 * norm_text_2)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id__left</th>\n",
       "      <th>text__left</th>\n",
       "      <th>text_embedded__left</th>\n",
       "      <th>id__right</th>\n",
       "      <th>text__right</th>\n",
       "      <th>text_embedded__right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46407</td>\n",
       "      <td>enhancement  current url querried another app ...</td>\n",
       "      <td>[-0.03849341, 0.044417124, 0.061231103, 0.0028...</td>\n",
       "      <td>516502</td>\n",
       "      <td>defect  add applescript support getting curren...</td>\n",
       "      <td>[-0.068924494, -0.0365692, 0.017821774, -0.005...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491005</td>\n",
       "      <td>defect powerpc applescript error get class cur...</td>\n",
       "      <td>[-0.042833928, -0.019467501, -0.0014609282, 0....</td>\n",
       "      <td>516502</td>\n",
       "      <td>defect  add applescript support getting curren...</td>\n",
       "      <td>[-0.068924494, -0.0365692, 0.017821774, -0.005...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95091</td>\n",
       "      <td>defect x86 helper application lack mozilla 0 9...</td>\n",
       "      <td>[-0.07100523, -0.028381774, 0.023365201, 0.037...</td>\n",
       "      <td>57420</td>\n",
       "      <td>defect  support helper app command line args u...</td>\n",
       "      <td>[0.030665655, -0.08451681, -0.060907457, -0.08...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>308815</td>\n",
       "      <td>defect x86 linux choose helper application all...</td>\n",
       "      <td>[-0.005184613, -0.028351225, -0.08792038, 0.01...</td>\n",
       "      <td>57420</td>\n",
       "      <td>defect  support helper app command line args u...</td>\n",
       "      <td>[0.030665655, -0.08451681, -0.060907457, -0.08...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>392205</td>\n",
       "      <td>enhancement  add support arguments help apps u...</td>\n",
       "      <td>[0.014655771, -0.03146617, -0.08758416, 0.0324...</td>\n",
       "      <td>57420</td>\n",
       "      <td>defect  support helper app command line args u...</td>\n",
       "      <td>[0.030665655, -0.08451681, -0.060907457, -0.08...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id__left                                         text__left  \\\n",
       "0     46407  enhancement  current url querried another app ...   \n",
       "1    491005  defect powerpc applescript error get class cur...   \n",
       "2     95091  defect x86 helper application lack mozilla 0 9...   \n",
       "3    308815  defect x86 linux choose helper application all...   \n",
       "4    392205  enhancement  add support arguments help apps u...   \n",
       "\n",
       "                                 text_embedded__left  id__right  \\\n",
       "0  [-0.03849341, 0.044417124, 0.061231103, 0.0028...     516502   \n",
       "1  [-0.042833928, -0.019467501, -0.0014609282, 0....     516502   \n",
       "2  [-0.07100523, -0.028381774, 0.023365201, 0.037...      57420   \n",
       "3  [-0.005184613, -0.028351225, -0.08792038, 0.01...      57420   \n",
       "4  [0.014655771, -0.03146617, -0.08758416, 0.0324...      57420   \n",
       "\n",
       "                                         text__right  \\\n",
       "0  defect  add applescript support getting curren...   \n",
       "1  defect  add applescript support getting curren...   \n",
       "2  defect  support helper app command line args u...   \n",
       "3  defect  support helper app command line args u...   \n",
       "4  defect  support helper app command line args u...   \n",
       "\n",
       "                                text_embedded__right  label  \n",
       "0  [-0.068924494, -0.0365692, 0.017821774, -0.005...      1  \n",
       "1  [-0.068924494, -0.0365692, 0.017821774, -0.005...      1  \n",
       "2  [0.030665655, -0.08451681, -0.060907457, -0.08...      1  \n",
       "3  [0.030665655, -0.08451681, -0.060907457, -0.08...      1  \n",
       "4  [0.030665655, -0.08451681, -0.060907457, -0.08...      1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_duplicates = pd.merge(left=df_duplicates,\n",
    "                        right=df_uniques,\n",
    "                        left_on='duplicates_to',\n",
    "                        right_on='id',\n",
    "                        suffixes=('__left', '__right'))\n",
    "df_duplicates = df_duplicates.drop(columns=['duplicates_to__left', 'duplicates_to__right'])\n",
    "df_duplicates['label'] = 1\n",
    "df_duplicates = df_duplicates.reset_index(drop=True)\n",
    "\n",
    "df_duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_embedded__left</th>\n",
       "      <th>text__left</th>\n",
       "      <th>id__left</th>\n",
       "      <th>text_embedded__right</th>\n",
       "      <th>text__right</th>\n",
       "      <th>id__right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.013469558, -0.010111869, 0.028897135, -0.00...</td>\n",
       "      <td>defect x86 search without dialog new search di...</td>\n",
       "      <td>264962</td>\n",
       "      <td>[-0.012708805, 0.036860116, 0.09029018, 0.1036...</td>\n",
       "      <td>defect x86 getelementsbytagname confused short...</td>\n",
       "      <td>293692.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.05257168, -0.01263137, 0.089446604, -0.017...</td>\n",
       "      <td>defect unspecified colour updates needed new t...</td>\n",
       "      <td>1451474</td>\n",
       "      <td>[0.013469558, -0.010111869, 0.028897135, -0.00...</td>\n",
       "      <td>defect x86 search without dialog new search di...</td>\n",
       "      <td>264962.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.0056334394, -0.014998359, 0.036152463, 0.0...</td>\n",
       "      <td>defect x86 64 one word urlbar searches incredi...</td>\n",
       "      <td>972452</td>\n",
       "      <td>[-0.05257168, -0.01263137, 0.089446604, -0.017...</td>\n",
       "      <td>defect unspecified colour updates needed new t...</td>\n",
       "      <td>1451474.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.03328692, -0.017311642, 0.02710121, -0.015...</td>\n",
       "      <td>defect x86 firefox installer handle directory ...</td>\n",
       "      <td>341797</td>\n",
       "      <td>[-0.0056334394, -0.014998359, 0.036152463, 0.0...</td>\n",
       "      <td>defect x86 64 one word urlbar searches incredi...</td>\n",
       "      <td>972452.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.04823823, -0.0018696992, -0.056124125, -0.0...</td>\n",
       "      <td>defect x86 firefox 18 0b1 b2 segmentation faul...</td>\n",
       "      <td>817482</td>\n",
       "      <td>[-0.03328692, -0.017311642, 0.02710121, -0.015...</td>\n",
       "      <td>defect x86 firefox installer handle directory ...</td>\n",
       "      <td>341797.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text_embedded__left  \\\n",
       "0  [0.013469558, -0.010111869, 0.028897135, -0.00...   \n",
       "1  [-0.05257168, -0.01263137, 0.089446604, -0.017...   \n",
       "2  [-0.0056334394, -0.014998359, 0.036152463, 0.0...   \n",
       "3  [-0.03328692, -0.017311642, 0.02710121, -0.015...   \n",
       "4  [0.04823823, -0.0018696992, -0.056124125, -0.0...   \n",
       "\n",
       "                                          text__left  id__left  \\\n",
       "0  defect x86 search without dialog new search di...    264962   \n",
       "1  defect unspecified colour updates needed new t...   1451474   \n",
       "2  defect x86 64 one word urlbar searches incredi...    972452   \n",
       "3  defect x86 firefox installer handle directory ...    341797   \n",
       "4  defect x86 firefox 18 0b1 b2 segmentation faul...    817482   \n",
       "\n",
       "                                text_embedded__right  \\\n",
       "0  [-0.012708805, 0.036860116, 0.09029018, 0.1036...   \n",
       "1  [0.013469558, -0.010111869, 0.028897135, -0.00...   \n",
       "2  [-0.05257168, -0.01263137, 0.089446604, -0.017...   \n",
       "3  [-0.0056334394, -0.014998359, 0.036152463, 0.0...   \n",
       "4  [-0.03328692, -0.017311642, 0.02710121, -0.015...   \n",
       "\n",
       "                                         text__right  id__right  label  \n",
       "0  defect x86 getelementsbytagname confused short...   293692.0      0  \n",
       "1  defect x86 search without dialog new search di...   264962.0      0  \n",
       "2  defect unspecified colour updates needed new t...  1451474.0      0  \n",
       "3  defect x86 64 one word urlbar searches incredi...   972452.0      0  \n",
       "4  defect x86 firefox installer handle directory ...   341797.0      0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_rows = df_uniques.shape[0] // 2\n",
    "df_uniques = df_uniques.sample(frac=1, replace=False, random_state=42)\n",
    "\n",
    "df_uniques_temp = pd.DataFrame()\n",
    "df_uniques_temp['text_embedded__left'] = df_uniques['text_embedded']\n",
    "df_uniques_temp['text__left'] = df_uniques['text']\n",
    "df_uniques_temp['id__left'] = df_uniques['id']\n",
    "\n",
    "df_uniques_temp['text_embedded__right'] = df_uniques['text_embedded'].shift(1)\n",
    "df_uniques_temp['text__right'] = df_uniques['text'].shift(1)\n",
    "df_uniques_temp['id__right'] = df_uniques['id'].shift(1)\n",
    "\n",
    "df_uniques_temp = df_uniques_temp.dropna()\n",
    "df_uniques_temp['label'] = 0\n",
    "\n",
    "df_uniques = df_uniques_temp.reset_index(drop=True)\n",
    "\n",
    "del df_uniques_temp\n",
    "gc.collect()\n",
    "\n",
    "df_uniques.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_duplicates, df_uniques], axis=0).sample(frac=1, replace=False, random_state=42).reset_index(drop=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Data Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1. Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (146384, 2)\n",
      "Shape of X_test: (62737, 2)\n",
      "Shape of y_train: (146384,)\n",
      "Shape of y_test: (62737,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[['text_embedded__left', 'text_embedded__right']],\n",
    "                                                    df['label'],\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=df['label'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Model Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 384)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 384)]                0         []                            \n",
      "                                                                                                  \n",
      " shared_node_1 (Dense)       (None, 256)                  98560     ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " shared_node_2 (Dense)       (None, 256)                  65792     ['shared_node_1[0][0]',       \n",
      "                                                                     'shared_node_1[1][0]']       \n",
      "                                                                                                  \n",
      " shared_node_3 (Dense)       (None, 128)                  32896     ['shared_node_2[0][0]',       \n",
      "                                                                     'shared_node_2[1][0]']       \n",
      "                                                                                                  \n",
      " cosine_similarity (Dot)     (None, 1)                    0         ['shared_node_3[0][0]',       \n",
      "                                                                     'shared_node_3[1][0]']       \n",
      "                                                                                                  \n",
      " output (Dense)              (None, 1)                    2         ['cosine_similarity[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 197250 (770.51 KB)\n",
      "Trainable params: 197250 (770.51 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_1 = Input(shape=(384, ), name='input_1')\n",
    "input_2 = Input(shape=(384, ), name='input_2')\n",
    "\n",
    "shared_node_1 = Dense(256, name='shared_node_1')\n",
    "shared_node_2 = Dense(256, name='shared_node_2')\n",
    "shared_node_3 = Dense(128, name='shared_node_3')\n",
    "\n",
    "x1 = shared_node_1(input_1)\n",
    "x1 = shared_node_2(x1)\n",
    "x1 = shared_node_3(x1)\n",
    "\n",
    "x2 = shared_node_1(input_2)\n",
    "x2 = shared_node_2(x2)\n",
    "x2 = shared_node_3(x2)\n",
    "\n",
    "cosine_similarity_layer = Dot(axes=-1, normalize=True, name='cosine_similarity')([x1, x2])\n",
    "output_layer = Dense(1, activation='sigmoid', name='output')(cosine_similarity_layer)\n",
    "\n",
    "training_model = Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4575/4575 [==============================] - 20s 4ms/step - loss: 0.3325 - auc_2: 0.9119 - val_loss: 0.1966 - val_auc_2: 0.9774\n",
      "Epoch 2/10\n",
      "4575/4575 [==============================] - 18s 4ms/step - loss: 0.1580 - auc_2: 0.9798 - val_loss: 0.1341 - val_auc_2: 0.9829\n",
      "Epoch 3/10\n",
      "4575/4575 [==============================] - 18s 4ms/step - loss: 0.1191 - auc_2: 0.9847 - val_loss: 0.1142 - val_auc_2: 0.9835\n",
      "Epoch 4/10\n",
      "4575/4575 [==============================] - 17s 4ms/step - loss: 0.1030 - auc_2: 0.9866 - val_loss: 0.1045 - val_auc_2: 0.9845\n",
      "Epoch 5/10\n",
      "4575/4575 [==============================] - 17s 4ms/step - loss: 0.0944 - auc_2: 0.9877 - val_loss: 0.1007 - val_auc_2: 0.9850\n",
      "Epoch 6/10\n",
      "4575/4575 [==============================] - 18s 4ms/step - loss: 0.0888 - auc_2: 0.9888 - val_loss: 0.0985 - val_auc_2: 0.9835\n",
      "Epoch 7/10\n",
      "4575/4575 [==============================] - 19s 4ms/step - loss: 0.0850 - auc_2: 0.9892 - val_loss: 0.0968 - val_auc_2: 0.9846\n",
      "Epoch 8/10\n",
      "4575/4575 [==============================] - 18s 4ms/step - loss: 0.0817 - auc_2: 0.9897 - val_loss: 0.0966 - val_auc_2: 0.9841\n",
      "Epoch 9/10\n",
      "4575/4575 [==============================] - 18s 4ms/step - loss: 0.0796 - auc_2: 0.9900 - val_loss: 0.0941 - val_auc_2: 0.9851\n",
      "Epoch 10/10\n",
      "4575/4575 [==============================] - 18s 4ms/step - loss: 0.0777 - auc_2: 0.9906 - val_loss: 0.0960 - val_auc_2: 0.9831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a8e947ef10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "training_model.fit(\n",
    "    [np.vstack(X_train['text_embedded__left']), np.vstack(X_train['text_embedded__right'])],\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=([np.vstack(X_test['text_embedded__left']), np.vstack(X_test['text_embedded__right'])], y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961/1961 [==============================] - 3s 1ms/step\n",
      "Precision: 0.8647\n",
      "Recall: 0.8483\n",
      "AUC: 0.9148\n"
     ]
    }
   ],
   "source": [
    "val_predictions = training_model.predict([np.vstack(X_test['text_embedded__left']), np.vstack(X_test['text_embedded__right'])])\n",
    "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "precision = precision_score(y_test, val_predictions_binary)\n",
    "recall = recall_score(y_test, val_predictions_binary)\n",
    "auc = roc_auc_score(y_test, val_predictions_binary)\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'AUC: {auc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
