{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msi.pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msi.pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "import concurrent.futures\n",
    "import asyncio\n",
    "import httpx\n",
    "import multiprocessing\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "from typing import List, Dict, Optional, Union\n",
    "from glob import glob\n",
    "\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import fasttext\n",
    "from gensim.models import KeyedVectors \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Input, Lambda, Dot\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url: str, params=None):\n",
    "  try:\n",
    "    response = requests.get(url, params=params)\n",
    "    if (response.status_code == 200):\n",
    "      return response.json()\n",
    "    \n",
    "    print(f\"Error response with status code: {response.status_code}\")\n",
    "  except Exception as error:\n",
    "    print(f'Failed to fetch data: {error}')\n",
    "\n",
    "def urls_builder(base_url: str, n_fetch: int, limit: int, products: List[str], **kwargs):\n",
    "  urls = []\n",
    "  for product in products:\n",
    "    for i in range(n_fetch):\n",
    "      \n",
    "      param = {\n",
    "        'offset': i * limit,\n",
    "        'limit': limit,\n",
    "        'product': product,\n",
    "        **kwargs,\n",
    "      }\n",
    "      \n",
    "      full_url = base_url + '?' + urlencode(param)\n",
    "      urls.append(full_url)\n",
    "    \n",
    "  return urls\n",
    "\n",
    "def save_json(data, path: str):\n",
    "  with open(path, 'w') as json_file:\n",
    "    json.dump(data, json_file)\n",
    "\n",
    "def load_json(path: str):\n",
    "  with open(path, 'r') as json_file:\n",
    "    loaded_data = json.load(json_file)\n",
    "  return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script skipped # Skipped due to resource optimization\n",
    "\n",
    "selected_fields = ['id', 'duplicates', 'summary', 'description', 'status', 'resolution', 'platform', 'product', 'type', 'priority', 'severity', 'component']\n",
    "products = ['Firefox']\n",
    "\n",
    "base_params = {\n",
    "  'include_fields': ', '.join(selected_fields),\n",
    "}\n",
    "saved_data_path = os.path.join('data', 'raw_data', 'firefox_raw_data.json')\n",
    "\n",
    "base_url = 'https://bugzilla.mozilla.org/rest/bug'\n",
    "n_fetch = 50\n",
    "limit = 5000\n",
    "\n",
    "if os.path.exists(saved_data_path):\n",
    "  response_data = load_json(saved_data_path)\n",
    "else:\n",
    "  urls = urls_builder(base_url, n_fetch, limit, products, **base_params)\n",
    "  response_data = []\n",
    "\n",
    "  max_workers = 50\n",
    "  with concurrent.futures.ThreadPoolExecutor(max_workers) as executor:\n",
    "    response_data = list(executor.map(fetch_data, urls))\n",
    "    \n",
    "  response_data = [item['bugs'] for item in response_data]\n",
    "  response_data = [item for sublist in response_data for item in sublist]\n",
    "  \n",
    "  save_json(response_data, saved_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = os.path.join('data', 'cache', 'raw_data.parquet')\n",
    "df = pd.DataFrame()\n",
    "\n",
    "if (os.path.exists(raw_data_path)):\n",
    "  df = pd.read_parquet(raw_data_path)\n",
    "else:\n",
    "  data_paths = glob(os.path.join('data', 'raw_data', 'firefox_raw_data.json'))\n",
    "\n",
    "  for path in data_paths:\n",
    "    data = load_json(path)\n",
    "    data = pd.DataFrame(data)\n",
    "    df = pd.concat([df, data])\n",
    "\n",
    "  df = df.set_index('id')\n",
    "  df.to_parquet(raw_data_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['status', 'priority', 'resolution', 'severity', 'component', 'product'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1. Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script skipped # Skipped due to resource optimization\n",
    "\n",
    "df['product'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3. Platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['platform'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4. Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5. Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df['duplicates'].apply(lambda x: len(x)).sort_values(ascending=False)\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_bugs = duplicates[duplicates > 0]\n",
    "duplicated_bugs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1. Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['type'] == '--', 'type'] = 'no type'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2. Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = df.loc[duplicated_bugs.index, 'duplicates']\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('data', 'cache', 'duplicate_to_data.parquet')\n",
    "\n",
    "if (os.path.exists(data_path)):\n",
    "  df = pd.read_parquet(data_path)\n",
    "else:\n",
    "  df['duplicates_to'] = -1\n",
    "\n",
    "  for idx, dups in zip(duplicated.index, duplicated):\n",
    "    for item in dups:\n",
    "      df.loc[df.index == item, 'duplicates_to'] = idx\n",
    "      \n",
    "  df.to_parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['duplicates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3. Clean Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text: str):\n",
    "  text = text.lower()\n",
    "  text = re.sub(r'\\n|\\t|\\r|\\0', ' ', text)\n",
    "  text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "  text = re.sub(r'\\s{2,}', ' ', text)\n",
    "  text = re.sub(r'\\s$', '', text)\n",
    "  text = re.sub(r'\\s[b-z]\\s', ' ', text)\n",
    "  text = re.sub(r'\\s[b-z]\\s', ' ', text)\n",
    "  \n",
    "  return text\n",
    "\n",
    "def remove_stopwords(text: str):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = word_tokenize(text)\n",
    "  filtered_words = [word for word in words if word not in stop_words]\n",
    "  \n",
    "  return ' '.join(filtered_words)\n",
    "\n",
    "def clean_text(text: str):\n",
    "  text = remove_special_chars(text)\n",
    "  text = remove_stopwords(text)\n",
    "  \n",
    "  return text\n",
    "\n",
    "df['platform'] = df['platform'].apply(clean_text)\n",
    "df['summary'] = df['summary'].apply(clean_text)\n",
    "df['type'] = df['type'].apply(clean_text)\n",
    "df['description'] = df['description'].apply(clean_text)\n",
    "\n",
    "# df['product'] = df['product'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4. Combined Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['type'] + ' ' + df['platform'] + ' ' + df['summary'] + ' ' + df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>duplicates_to</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10954</th>\n",
       "      <td>enhancement  dialup properties needs exposed p...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14871</th>\n",
       "      <td>defect  find find whole word please add match ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19118</th>\n",
       "      <td>enhancement  plug manager ui choosing mimetype...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21482</th>\n",
       "      <td>enhancement  improvement save file dialog fold...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23207</th>\n",
       "      <td>enhancement unspecified options save location ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  duplicates_to\n",
       "id                                                                     \n",
       "10954  enhancement  dialup properties needs exposed p...             -1\n",
       "14871  defect  find find whole word please add match ...             -1\n",
       "19118  enhancement  plug manager ui choosing mimetype...             -1\n",
       "21482  enhancement  improvement save file dialog fold...             -1\n",
       "23207  enhancement unspecified options save location ...             -1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['text', 'duplicates_to']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.5. Sentence Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script skipped # Skipped\n",
    "\n",
    "sent_embd_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script skipped # Skipped\n",
    "\n",
    "embd_data_path = os.path.join('data', 'cache', 'sent_trans_embd_data.parquet')\n",
    "\n",
    "if (os.path.exists(embd_data_path)):\n",
    "  df = pd.read_parquet(embd_data_path)\n",
    "else:\n",
    "  df['text_embedded'] = df['text'].apply(sent_embd_model.encode)\n",
    "  df.to_parquet(embd_data_path)\n",
    "\n",
    "df['text_embedded'] = df['text_embedded'].apply(np.array)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script skipped # Skipped\n",
    "\n",
    "model_path = os.path.join('models', 'wiki-news-300d-1M-subword.vec')\n",
    "sent_embd_model = KeyedVectors.load_word2vec_format(model_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script skipped # Skipped\n",
    "\n",
    "def sentence_embedding(sentence, model):\n",
    "    words = word_tokenize(sentence)\n",
    "    words_in_vocab = [word for word in words if word in model]\n",
    "\n",
    "    if not words_in_vocab:\n",
    "        return None\n",
    "\n",
    "    embedding = np.mean([model[word] for word in words_in_vocab], axis=0)\n",
    "    embedding = normalize(embedding.reshape(1, -1), norm='l2').reshape(-1)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%script skipped # Skipped\n",
    "\n",
    "embd_data_path = os.path.join('data', 'cache', 'fasttext_embd_data.parquet')\n",
    "\n",
    "if (os.path.exists(embd_data_path)):\n",
    "  df = pd.read_parquet(embd_data_path)\n",
    "else:\n",
    "  df['text_embedded'] = df['text'].apply(sentence_embedding, model=sent_embd_model)\n",
    "  df.to_parquet(embd_data_path)\n",
    "\n",
    "df['text_embedded'] = df['text_embedded'].apply(np.array)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 210693 entries, 10954 to 1876223\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   text           210693 non-null  object\n",
      " 1   duplicates_to  210693 non-null  int64 \n",
      " 2   text_embedded  210693 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 450.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.6. Sentence Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicates = df[df['duplicates_to'] != -1].copy()\n",
    "df_uniques = df[df['duplicates_to'] == -1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cosine_similarity(text_1: pd.Series, text_2: pd.Series):\n",
    "    dot_product = np.dot(text_1, text_2)\n",
    "    norm_text_1 = np.linalg.norm(text_1)\n",
    "    norm_text_2 = np.linalg.norm(text_2)\n",
    "    \n",
    "    similarity = dot_product / (norm_text_1 * norm_text_2)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id__left</th>\n",
       "      <th>text__left</th>\n",
       "      <th>duplicates_to__left</th>\n",
       "      <th>text_embedded__left</th>\n",
       "      <th>id__right</th>\n",
       "      <th>text__right</th>\n",
       "      <th>duplicates_to__right</th>\n",
       "      <th>text_embedded__right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46407</td>\n",
       "      <td>enhancement  current url querried another app ...</td>\n",
       "      <td>516502</td>\n",
       "      <td>[-0.0023696667, -0.035115827, 0.062171057, -0....</td>\n",
       "      <td>516502</td>\n",
       "      <td>defect  add applescript support getting curren...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.00920859, 0.014499452, 0.032967843, 0.0078...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491005</td>\n",
       "      <td>defect powerpc applescript error get class cur...</td>\n",
       "      <td>516502</td>\n",
       "      <td>[-0.006655017, 0.028761698, 0.03554143, 0.0072...</td>\n",
       "      <td>516502</td>\n",
       "      <td>defect  add applescript support getting curren...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.00920859, 0.014499452, 0.032967843, 0.0078...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95091</td>\n",
       "      <td>defect x86 helper application lack mozilla 0 9...</td>\n",
       "      <td>57420</td>\n",
       "      <td>[-0.03253318, 0.011125067, 0.031311132, 0.0223...</td>\n",
       "      <td>57420</td>\n",
       "      <td>defect  support helper app command line args u...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.009637785, 0.00013978098, 0.023175225, 0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>308815</td>\n",
       "      <td>defect x86 linux choose helper application all...</td>\n",
       "      <td>57420</td>\n",
       "      <td>[0.0041687232, 0.02763497, 0.012876986, 0.0068...</td>\n",
       "      <td>57420</td>\n",
       "      <td>defect  support helper app command line args u...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.009637785, 0.00013978098, 0.023175225, 0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>392205</td>\n",
       "      <td>enhancement  add support arguments help apps u...</td>\n",
       "      <td>57420</td>\n",
       "      <td>[-0.0012838916, 0.024868222, 0.021619998, 0.00...</td>\n",
       "      <td>57420</td>\n",
       "      <td>defect  support helper app command line args u...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.009637785, 0.00013978098, 0.023175225, 0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id__left                                         text__left  \\\n",
       "0     46407  enhancement  current url querried another app ...   \n",
       "1    491005  defect powerpc applescript error get class cur...   \n",
       "2     95091  defect x86 helper application lack mozilla 0 9...   \n",
       "3    308815  defect x86 linux choose helper application all...   \n",
       "4    392205  enhancement  add support arguments help apps u...   \n",
       "\n",
       "   duplicates_to__left                                text_embedded__left  \\\n",
       "0               516502  [-0.0023696667, -0.035115827, 0.062171057, -0....   \n",
       "1               516502  [-0.006655017, 0.028761698, 0.03554143, 0.0072...   \n",
       "2                57420  [-0.03253318, 0.011125067, 0.031311132, 0.0223...   \n",
       "3                57420  [0.0041687232, 0.02763497, 0.012876986, 0.0068...   \n",
       "4                57420  [-0.0012838916, 0.024868222, 0.021619998, 0.00...   \n",
       "\n",
       "   id__right                                        text__right  \\\n",
       "0     516502  defect  add applescript support getting curren...   \n",
       "1     516502  defect  add applescript support getting curren...   \n",
       "2      57420  defect  support helper app command line args u...   \n",
       "3      57420  defect  support helper app command line args u...   \n",
       "4      57420  defect  support helper app command line args u...   \n",
       "\n",
       "   duplicates_to__right                               text_embedded__right  \\\n",
       "0                    -1  [-0.00920859, 0.014499452, 0.032967843, 0.0078...   \n",
       "1                    -1  [-0.00920859, 0.014499452, 0.032967843, 0.0078...   \n",
       "2                    -1  [-0.009637785, 0.00013978098, 0.023175225, 0.0...   \n",
       "3                    -1  [-0.009637785, 0.00013978098, 0.023175225, 0.0...   \n",
       "4                    -1  [-0.009637785, 0.00013978098, 0.023175225, 0.0...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_duplicates = pd.merge(left=df_duplicates,\n",
    "                        right=df_uniques,\n",
    "                        left_on='duplicates_to',\n",
    "                        right_on='id',\n",
    "                        suffixes=('__left', '__right'))\n",
    "df_duplicates['label'] = 1\n",
    "df_duplicates = df_duplicates.reset_index(drop=True)\n",
    "\n",
    "df_duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_embedded__left</th>\n",
       "      <th>text__left</th>\n",
       "      <th>id__left</th>\n",
       "      <th>text_embedded__right</th>\n",
       "      <th>text__right</th>\n",
       "      <th>id__right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0064705536, 0.045026768, 0.014774143, 0.009...</td>\n",
       "      <td>defect x86 search without dialog new search di...</td>\n",
       "      <td>264962</td>\n",
       "      <td>[0.015322351, 0.039216094, 0.018340139, 0.0095...</td>\n",
       "      <td>defect x86 getelementsbytagname confused short...</td>\n",
       "      <td>293692.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0075292224, 0.022191392, 0.05013739, 0.0050...</td>\n",
       "      <td>defect unspecified colour updates needed new t...</td>\n",
       "      <td>1451474</td>\n",
       "      <td>[0.0064705536, 0.045026768, 0.014774143, 0.009...</td>\n",
       "      <td>defect x86 search without dialog new search di...</td>\n",
       "      <td>264962.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0017332855, 0.017958546, 0.028890675, 0.011...</td>\n",
       "      <td>defect x86 64 one word urlbar searches incredi...</td>\n",
       "      <td>972452</td>\n",
       "      <td>[0.0075292224, 0.022191392, 0.05013739, 0.0050...</td>\n",
       "      <td>defect unspecified colour updates needed new t...</td>\n",
       "      <td>1451474.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.0018321006, -0.034610473, 0.020652002, -0....</td>\n",
       "      <td>defect x86 firefox installer handle directory ...</td>\n",
       "      <td>341797</td>\n",
       "      <td>[0.0017332855, 0.017958546, 0.028890675, 0.011...</td>\n",
       "      <td>defect x86 64 one word urlbar searches incredi...</td>\n",
       "      <td>972452.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0012329025, 0.0037409784, 0.01816242, 0.001...</td>\n",
       "      <td>defect x86 firefox 18 0b1 b2 segmentation faul...</td>\n",
       "      <td>817482</td>\n",
       "      <td>[-0.0018321006, -0.034610473, 0.020652002, -0....</td>\n",
       "      <td>defect x86 firefox installer handle directory ...</td>\n",
       "      <td>341797.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text_embedded__left  \\\n",
       "0  [0.0064705536, 0.045026768, 0.014774143, 0.009...   \n",
       "1  [0.0075292224, 0.022191392, 0.05013739, 0.0050...   \n",
       "2  [0.0017332855, 0.017958546, 0.028890675, 0.011...   \n",
       "3  [-0.0018321006, -0.034610473, 0.020652002, -0....   \n",
       "4  [0.0012329025, 0.0037409784, 0.01816242, 0.001...   \n",
       "\n",
       "                                          text__left  id__left  \\\n",
       "0  defect x86 search without dialog new search di...    264962   \n",
       "1  defect unspecified colour updates needed new t...   1451474   \n",
       "2  defect x86 64 one word urlbar searches incredi...    972452   \n",
       "3  defect x86 firefox installer handle directory ...    341797   \n",
       "4  defect x86 firefox 18 0b1 b2 segmentation faul...    817482   \n",
       "\n",
       "                                text_embedded__right  \\\n",
       "0  [0.015322351, 0.039216094, 0.018340139, 0.0095...   \n",
       "1  [0.0064705536, 0.045026768, 0.014774143, 0.009...   \n",
       "2  [0.0075292224, 0.022191392, 0.05013739, 0.0050...   \n",
       "3  [0.0017332855, 0.017958546, 0.028890675, 0.011...   \n",
       "4  [-0.0018321006, -0.034610473, 0.020652002, -0....   \n",
       "\n",
       "                                         text__right  id__right  label  \n",
       "0  defect x86 getelementsbytagname confused short...   293692.0      0  \n",
       "1  defect x86 search without dialog new search di...   264962.0      0  \n",
       "2  defect unspecified colour updates needed new t...  1451474.0      0  \n",
       "3  defect x86 64 one word urlbar searches incredi...   972452.0      0  \n",
       "4  defect x86 firefox installer handle directory ...   341797.0      0  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_rows = df_uniques.shape[0] // 2\n",
    "df_uniques = df_uniques.sample(frac=1, replace=False, random_state=42)\n",
    "\n",
    "df_uniques_temp = pd.DataFrame()\n",
    "df_uniques_temp['text_embedded__left'] = df_uniques['text_embedded']\n",
    "df_uniques_temp['text__left'] = df_uniques['text']\n",
    "df_uniques_temp['id__left'] = df_uniques['id']\n",
    "\n",
    "df_uniques_temp['text_embedded__right'] = df_uniques['text_embedded'].shift(1)\n",
    "df_uniques_temp['text__right'] = df_uniques['text'].shift(1)\n",
    "df_uniques_temp['id__right'] = df_uniques['id'].shift(1)\n",
    "\n",
    "df_uniques_temp = df_uniques_temp.dropna()\n",
    "df_uniques_temp['label'] = 0\n",
    "\n",
    "df_uniques = df_uniques_temp.reset_index(drop=True)\n",
    "\n",
    "del df_uniques_temp\n",
    "gc.collect()\n",
    "\n",
    "df_uniques.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1596"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_duplicates, df_uniques], axis=0).sample(frac=1, replace=False, random_state=42).reset_index(drop=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Data Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1. Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (146384, 2)\n",
      "Shape of X_test: (62737, 2)\n",
      "Shape of y_train: (146384,)\n",
      "Shape of y_test: (62737,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[['text_embedded__left', 'text_embedded__right']],\n",
    "                                                    df['label'],\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=df['label'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Model Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 300)]                0         []                            \n",
      "                                                                                                  \n",
      " shared_node_1 (Dense)       (None, 256)                  77056     ['input_1[0][0]',             \n",
      "                                                                     'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " shared_node_2 (Dense)       (None, 256)                  65792     ['shared_node_1[0][0]',       \n",
      "                                                                     'shared_node_1[1][0]']       \n",
      "                                                                                                  \n",
      " shared_node_3 (Dense)       (None, 128)                  32896     ['shared_node_2[0][0]',       \n",
      "                                                                     'shared_node_2[1][0]']       \n",
      "                                                                                                  \n",
      " cosine_similarity (Dot)     (None, 1)                    0         ['shared_node_3[0][0]',       \n",
      "                                                                     'shared_node_3[1][0]']       \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 300)]                0         []                            \n",
      "                                                                                                  \n",
      " output (Dense)              (None, 1)                    2         ['cosine_similarity[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 175746 (686.51 KB)\n",
      "Trainable params: 175746 (686.51 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_1 = Input(shape=(300, ), name='input_1')\n",
    "input_2 = Input(shape=(300, ), name='input_2')\n",
    "\n",
    "shared_node_1 = Dense(256, name='shared_node_1')\n",
    "shared_node_2 = Dense(256, name='shared_node_2')\n",
    "shared_node_3 = Dense(128, name='shared_node_3')\n",
    "\n",
    "x1 = shared_node_1(input_1)\n",
    "x1 = shared_node_2(x1)\n",
    "x1 = shared_node_3(x1)\n",
    "\n",
    "x2 = shared_node_1(input_1)\n",
    "x2 = shared_node_2(x2)\n",
    "x2 = shared_node_3(x2)\n",
    "\n",
    "cosine_similarity_layer = Dot(axes=-1, normalize=True, name='cosine_similarity')([x1, x2])\n",
    "output_layer = Dense(1, activation='sigmoid', name='output')(cosine_similarity_layer)\n",
    "\n",
    "training_model = Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 300)]                0         []                            \n",
      "                                                                                                  \n",
      " shared_node_1 (Dense)       (None, 256)                  77056     ['input_1[0][0]',             \n",
      "                                                                     'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " shared_node_2 (Dense)       (None, 256)                  65792     ['shared_node_1[0][0]',       \n",
      "                                                                     'shared_node_1[1][0]']       \n",
      "                                                                                                  \n",
      " shared_node_3 (Dense)       (None, 128)                  32896     ['shared_node_2[0][0]',       \n",
      "                                                                     'shared_node_2[1][0]']       \n",
      "                                                                                                  \n",
      " cosine_similarity (Dot)     (None, 1)                    0         ['shared_node_3[0][0]',       \n",
      "                                                                     'shared_node_3[1][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 175744 (686.50 KB)\n",
      "Trainable params: 175744 (686.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inference_model = Model(inputs=input_1, outputs=cosine_similarity_layer)\n",
    "inference_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From d:\\Programs\\Miniconda3\\envs\\env_1\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "4575/4575 [==============================] - 17s 3ms/step - loss: 0.5700 - precision_6: 0.1251 - recall_6: 0.2199\n",
      "Epoch 2/10\n",
      "4575/4575 [==============================] - 14s 3ms/step - loss: 0.3738 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4575/4575 [==============================] - 15s 3ms/step - loss: 0.3738 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4575/4575 [==============================] - 15s 3ms/step - loss: 0.3738 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4575/4575 [==============================] - 16s 4ms/step - loss: 0.3738 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4575/4575 [==============================] - 15s 3ms/step - loss: 0.3738 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4575/4575 [==============================] - 15s 3ms/step - loss: 0.3738 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4575/4575 [==============================] - 15s 3ms/step - loss: 0.3738 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4575/4575 [==============================] - 15s 3ms/step - loss: 0.3738 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4575/4575 [==============================] - 15s 3ms/step - loss: 0.3738 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17f82df8c50>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "training_model.fit(\n",
    "    [np.vstack(X_train['text_embedded__left']), np.vstack(X_train['text_embedded__right'])],\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961/1961 [==============================] - 4s 2ms/step\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Miniconda3\\envs\\env_1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "val_predictions = training_model.predict([np.vstack(X_test['text_embedded__left']), np.vstack(X_test['text_embedded__right'])])\n",
    "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "precision = precision_score(y_test, val_predictions_binary)\n",
    "recall = recall_score(y_test, val_predictions_binary)\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
